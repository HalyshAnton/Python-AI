{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3/MLrQ4EZLxUdWAeliw8I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalyshAnton/Python-AI/blob/AI_6_lesson/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Посібник для початківців з LangChain\n",
        "\n",
        "LangChain - це надійний фреймворк, призначений для створення додатків на основі **великих мовних моделей (LLMs)**. Він спрощує робочі процеси, інтегруючи різні компоненти, такі як підказки, пам'ять і зовнішні інструменти, дозволяючи розробникам створювати інтелектуальні програми швидко і ефективно."
      ],
      "metadata": {
        "id": "P06V9UGoyru1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Як використовувати LLM в LangChain**\n",
        "\n",
        "### Крок 1: Встановлення необхідних бібліотек\n"
      ],
      "metadata": {
        "id": "OcFhfVPczGMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community huggingface_hub langchain_huggingface"
      ],
      "metadata": {
        "id": "W5aS4QguzOHE",
        "outputId": "6c29cd12-5b25-4dbc-dbf7-2283c920a093",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Крок 2: Отримайте токен API Hugging Face\n",
        "Щоб використовувати моделі, розміщені на Hugging Face, вам потрібен токен API. Отримайте його з [Hugging Face](https://huggingface.co/settings/tokens)."
      ],
      "metadata": {
        "id": "qUJgoD9RzZJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''"
      ],
      "metadata": {
        "id": "56tM-BD1za3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Крок 3: Ініціалізація моделі Mistral-7B-Instruct-v0.2\n",
        "Використовуйте інтеграцію Hugging Face для завантаження моделі **mistralai/Mistral-7B-Instruct-v0.2**."
      ],
      "metadata": {
        "id": "kcJUB9_uzmYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Ініціалізуємо бібліотеку LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.7, # Налаштуйте креативність\n",
        "        \"max_length\": 256, # Обмежити довжину відповіді\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Протестуйте модель за допомогою простого запиту\n",
        "prompt = 'What is LangChain?'\n",
        "response = llm(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtpoLJAszp6M",
        "outputId": "621f4e37-42a0-4931-8957-10290472e944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is LangChain?\n",
            "\n",
            "LangChain is a decentralized, open-source, and blockchain-based AI language model. It is designed to provide high-quality and real-time language translation services, including text summarization, text generation, and question answering. LangChain utilizes the power of distributed computing and the security of the blockchain to create a decentralized language model that is more secure, transparent, and inclusive than traditional centralized models.\n",
            "\n",
            "How does LangChain work?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Параметри моделі: Детальний опис**\n",
        "\n",
        "При взаємодії з LLM параметри тонкої настройки можуть суттєво впливати на результати роботи моделі. Ось детальне пояснення:"
      ],
      "metadata": {
        "id": "rjbdqNHWz45l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Температура**\n",
        "- **Що робить:** Керує випадковістю у вихідних даних.\n",
        "- **Діапазон:** Від 0.0 до 1.0 (або вище).\n",
        "- **Низькі значення (наприклад, 0.2):** Модель генерує детерміновані та цілеспрямовані відповіді.\n",
        "- **Високі значення (наприклад, 0,8):** Модель дає різноманітні та творчі результати."
      ],
      "metadata": {
        "id": "JctJgVfaz_Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Max Tokens**\n",
        "- **Що робить:** Обмежує максимальну кількість токенів (слів/частин слів) у виведенні.\n",
        "- **Використання:** Запобігає отриманню надто довгих або урізаних відповідей."
      ],
      "metadata": {
        "id": "IU6Njib20C_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Top-p (вибірка ядра)**.\n",
        "- **Що робить:** Обмежує вибірку токенів до максимальної кумулятивної ймовірності `p`.\n",
        "- Діапазон:** від 0.0 до 1.0.\n",
        "- **Низькі значення (наприклад, 0.3):** Вихідні дані є більш детермінованими.\n",
        "- **Високі значення (наприклад, 0.9):** Результат є більш творчим."
      ],
      "metadata": {
        "id": "ZZp988jY0F5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Frequency Penalty**\n",
        "- **Що він робить:** Штрафує токени, які вже з'являлися, щоб заохотити різноманітні відповіді.\n",
        "- **Діапазон:** від -2.0 до 2.0.\n",
        "- **Вищі значення:** Перешкоджають повторенню.\n",
        "- **Нижчі значення:** Дозволяють більше повторень."
      ],
      "metadata": {
        "id": "MVsvMrGc0MO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Presence Penalty**\n",
        "- **Що робить:** Штрафує токени на основі того, чи з'являються вони у вхідних даних.\n",
        "- **Використання:** Заохочує вводити нові поняття у відповідях.\n",
        "- **Діапазон:** від -2.0 до 2.0."
      ],
      "metadata": {
        "id": "7EpnbTC90QR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Приклад: Об'єднання параметрів**\n",
        "Ось як ви можете комбінувати різні параметри для налаштування поведінки:"
      ],
      "metadata": {
        "id": "kr-zzTH80WAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.6,      # Збалансована креативність\n",
        "        \"max_length\": 200,       # Відповіді середньої довжини\n",
        "        \"top_p\": 0.85,           # Креативна вибірка\n",
        "        \"frequency_penalty\": 0.5 # Заохочуємо різноманітність\n",
        "    }\n",
        ")\n",
        "\n",
        "response = llm(\"Write short poem about moon\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ5ERy6f0cVl",
        "outputId": "3cb53066-e7cd-46c5-9b20-a6679615bd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write short poem about moon and stars\n",
            "\n",
            "Title: Lunar Serenade\n",
            "\n",
            "Beneath the twilight's gentle veil,\n",
            "The moon ascends her throne,\n",
            "A regal orb in tranquil sail,\n",
            "In celestial dance, alone.\n",
            "\n",
            "Her silver light, a tender glow,\n",
            "Bestows upon the night,\n",
            "A ballet of stars in tow,\n",
            "In ethereal radiance, bright.\n",
            "\n",
            "They twinkle, they dance, they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Що таке prompt і як його використовувати?**\n",
        "\n",
        "Підказка - це вхідні дані або інструкції, які ви даєте LLM, щоб спрямувати його реакцію. Складання правильної підказки має важливе значення для отримання значущих і точних результатів."
      ],
      "metadata": {
        "id": "7a-8EielsNr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Як використовувати підказку в LangChain\n",
        "Використовувати підказки в LangChain дуже просто. Ось базовий приклад:"
      ],
      "metadata": {
        "id": "l8SslwdlsPqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize the model\n",
        "llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "                     model_kwargs={\"temperature\": 0.7})\n",
        "\n",
        "# Define a prompt\n",
        "prompt = \"Explain the benefits of artificial intelligence in education.\"\n",
        "\n",
        "# Get the model's response\n",
        "response = llm(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XaPntFBsfGH",
        "outputId": "dcab19be-25d0-4107-bb08-95598755d2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain the benefits of artificial intelligence in education.\n",
            "\n",
            "Artificial Intelligence (AI) is transforming various industries, and education is no exception. AI technologies have the potential to revolutionize the way we learn and teach, bringing numerous benefits to students, teachers, and educational institutions. Here are some key advantages of using AI in education:\n",
            "\n",
            "1. Personalized Learning: AI-powered education systems can analyze a student's learning patterns, strengths, weaknesses, and interests to provide personalized learning experiences. This approach can help students learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Поради та приклади створення підказок**\n",
        "\n",
        "Створення ефективних підказок вимагає уваги до чіткості, контексту і бажаного формату виводу. Нижче наведено кілька порад і прикладів:"
      ],
      "metadata": {
        "id": "6IVvHKwKskLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Поради щодо створення підказок**\n",
        "1. **Будьте конкретними:** Чітко сформулюйте, що ви хочете, щоб LLM зробив.\n",
        "2. **Забезпечте контекст:** Надайте моделі відповідну довідкову інформацію, щоб покращити її реакцію.\n",
        "3. **Визначте вихідний формат:** Вкажіть, чи потрібен вам список, таблиця або структурована відповідь.\n",
        "4. **Експериментуйте:** Тестуйте та вдосконалюйте свої підказки для отримання кращих результатів."
      ],
      "metadata": {
        "id": "J9ReJpmcspD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Приклад 1: Проста інструкція**"
      ],
      "metadata": {
        "id": "CX7w0W74sroh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Describe the role of AI in healthcare in two sentences.\"\n",
        "response = llm(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOR6_L_9syWB",
        "outputId": "60505ddf-4c63-4d0e-887e-f166f2f38320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe the role of AI in healthcare in two sentences. AI is revolutionizing healthcare by analyzing vast amounts of patient data and medical literature to identify patterns, make predictions, and suggest personalized treatment plans. It is also improving operational efficiency, reducing costs, and enhancing the patient experience through automated scheduling, triage, and virtual assistants.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Приклад 2: Структуроване виведення**"
      ],
      "metadata": {
        "id": "htT5-dristsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "List three benefits of AI in education:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\"\"\"\n",
        "response = llm(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRDe_2D0s0jB",
        "outputId": "2dcbeade-2df8-418f-f86e-2f652c013d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "List three benefits of AI in education:\n",
            "1.\n",
            "2.\n",
            "3.\n",
            "1. Personalized Learning: AI can analyze a student's learning patterns, strengths, and weaknesses, and provide customized lesson plans and resources to help them learn more effectively. This can lead to improved academic performance and better overall learning experiences.\n",
            "2. Automated Administrative Tasks: AI can automate administrative tasks such as grading, scheduling, and record-keeping. This can save teachers time and allow them to focus more on teaching and interacting with students.\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Приклад 3: Контекстно-керований запит**"
      ],
      "metadata": {
        "id": "qk_LxZPasxSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are a customer support chatbot.\n",
        "Answer the following question in a friendly and concise tone:\n",
        "What are the refund policies for your service?\n",
        "\"\"\"\n",
        "response = llm(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khE8a5nVs4PI",
        "outputId": "b541f53b-6b8b-4483-8874-f91dea24d8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a customer support chatbot.\n",
            "Answer the following question in a friendly and concise tone:\n",
            "What are the refund policies for your service?\n",
            "Hello there! I'd be happy to help answer your question about our refund policies. We believe in providing the best possible service to our customers, but we also understand that sometimes things don't work out as planned. If you're not satisfied with our service within the first 30 days, we offer a full refund with no questions asked. After 30 days, we'll evaluate each refund request on a case-by-case basis and do our best to find a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Розширена обробка запитів за допомогою шаблонів запитів**\n",
        "LangChain надає ``PromptTemplate`` для динамічної генерації підказок із заповнювачами:"
      ],
      "metadata": {
        "id": "n19lMuins9MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a template with placeholders\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"length\"],\n",
        "    template=\"Write a {length} explanation about {topic}.\"\n",
        ")\n",
        "\n",
        "# Fill in the placeholders\n",
        "filled_prompt = template.format(topic=\"machine learning\", length=\"brief\")\n",
        "response = llm(filled_prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JavSUe5tA15",
        "outputId": "b99b267a-f845-4c4b-b967-ad3aed85f88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a brief explanation about machine learning.\n",
            "\n",
            "Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
            "\n",
            "The process of learning begins with feeding the machine learning algorithm a large set of data. The algorithm then uses statistical analysis, pattern recognition, and other techniques to identify and learn the underlying structures and relationships within the data. Once the algorithm has learned these patterns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Допоміжні слова**\n",
        "\n",
        "LangChain підтримує структуровані підказки зі спеціальними «допоміжними словами» або сигналами форматування для надання чітких інструкцій. Ось що означають деякі з них:\n",
        "\n",
        "- **`<< FORMATTING >>`:** Вказує бажану структуру виводу, наприклад, списки, маркери або таблиці.\n",
        "- **`<< CANDIDATE PROMPTS >>`:** Пропонує можливі варіанти підказки для експериментування або уточнення.\n",
        "- **`<< INPUT >>`:** Вказує, де у підказку буде вставлено динамічне введення (наприклад, дані, надані користувачем).\n",
        "- **`REMEMBER`:** Вказує моделі зберігати ключову інформацію для безперервності відповідей."
      ],
      "metadata": {
        "id": "fSOXWTu8OiBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Приклади з допоміжними словами**"
      ],
      "metadata": {
        "id": "-upVdm4oOyKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize the model\n",
        "llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "                     model_kwargs={\"temperature\": 0.7})\n",
        "\n",
        "# Prompt with formatting instruction\n",
        "prompt = \"\"\"\n",
        "Explain the advantages of renewable energy.\n",
        "<< FORMATTING >>\n",
        "- List three key points.\n",
        "\"\"\"\n",
        "\n",
        "response = llm(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR8mInCsO5fu",
        "outputId": "15af0cc7-fa60-4fc4-bf6f-52591603edfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explain the advantages of renewable energy.\n",
            "<< FORMATTING >>\n",
            "- List three key points.\n",
            "1. **Reliance on Renewable Resources:** Renewable energy comes from naturally occurring sources like sunlight, wind, rain, geothermal heat, and tidal power. Unlike non-renewable energy sources like coal and natural gas, which are finite and will eventually run out, renewable energy sources are virtually inexhaustible.\n",
            "2. **Environmental Sustainability:** Renewable energy is a clean source of energy that produces little to no greenhouse gas emissions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Create a dynamic prompt template\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Describe the importance of {topic} in modern society. << INPUT >>\"\n",
        ")\n",
        "\n",
        "# Generate a prompt\n",
        "prompt = template.format(topic=\"artificial intelligence\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6f7SxO8O6JY",
        "outputId": "fc8e2de3-e0dd-4467-ff52-7859852d9fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Describe the importance of artificial intelligence in modern society. << INPUT >>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You are a helpful assistant.\n",
        "\n",
        "REMEMBER: Be concise and avoid overly technical language.\n",
        "\n",
        "Answer the question: What is blockchain technology?\n",
        "\"\"\"\n",
        "response = llm(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDpG-_ljO9KR",
        "outputId": "e0d3b7cd-8847-4f82-f6ec-65ac489bff15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant.\n",
            "\n",
            "REMEMBER: Be concise and avoid overly technical language.\n",
            "\n",
            "Answer the question: What is blockchain technology?\n",
            "\n",
            "Blockchain technology is a type of digital ledger where transactions are recorded in a secure and unalterable way. It allows for the creation of a decentralized system where each participant has a copy of the entire transaction history. Once data has been recorded, it cannot be altered or deleted without the consensus of the network. This ensures transparency, security, and integrity of the data. It is most commonly known for being the technology behind cryptocurrencies like Bitcoin, but its applications extend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ланцюги у LangChain**\n",
        "\n",
        "Ланцюжки - це робочі процеси, у яких вихідні дані одного кроку надходять на наступний. LangChain надає інструменти для створення **простих ланцюжків** та більш складних **маршрутизаторів**."
      ],
      "metadata": {
        "id": "iPmC2i5pPH5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Прості ланцюжки**\n",
        "\n",
        "Простий ланцюжок складається з одного запиту і LLM, з додатковою пам'яттю."
      ],
      "metadata": {
        "id": "OB_php12PKLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define a simple prompt template\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Write a friendly greeting for {name}.\"\n",
        ")\n",
        "\n",
        "# Create a simple chain\n",
        "chain = LLMChain(llm=llm, prompt=template)\n",
        "\n",
        "# Run the chain\n",
        "response = chain.run(name=\"Alice\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJdNaE7bPL8Z",
        "outputId": "4c7d1d30-dd6e-4560-d441-e4e74f19bc3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-775bcb1aa1cd>:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=template)\n",
            "<ipython-input-13-775bcb1aa1cd>:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chain.run(name=\"Alice\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a friendly greeting for Alice.\n",
            "\n",
            "Hello Alice, it's lovely to see you here today! I hope you're having a wonderful day so far. I'm here to help answer any questions you might have or just chat about anything that's on your mind. Don't hesitate to reach out and let me know how I can make your day even better. Welcome once again to our friendly community! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для робити з ланцюгами потрібно використовувати **ChatModel**"
      ],
      "metadata": {
        "id": "ScBebDRkdBdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
        "\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    BaseMessage,\n",
        "    FunctionMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        "    ToolMessage,\n",
        ")\n",
        "\n",
        "from langchain_core.callbacks import (\n",
        "    AsyncCallbackManagerForLLMRun,\n",
        "    CallbackManagerForLLMRun,\n",
        ")\n",
        "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
        "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from langchain_core.runnables import run_in_executor\n",
        "\n",
        "\n",
        "class CustomChatModelAdvanced(BaseChatModel):\n",
        "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
        "\n",
        "    When contributing an implementation to LangChain, carefully document\n",
        "    the model including the initialization parameters, include\n",
        "    an example of how to initialize the model and include any relevant\n",
        "    links to the underlying models documentation or API.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        .. code-block:: python\n",
        "\n",
        "            model = CustomChatModel(n=2)\n",
        "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
        "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
        "                                 [HumanMessage(content=\"world\")]])\n",
        "    \"\"\"\n",
        "\n",
        "    model: HuggingFaceHub\n",
        "    \"\"\"The name of the model\"\"\"\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"Override the _generate method to implement the chat model logic.\n",
        "\n",
        "        This can be a call to an API, a call to a local model, or any other\n",
        "        implementation that generates a response to the input prompt.\n",
        "\n",
        "        Args:\n",
        "            messages: the prompt composed of a list of messages.\n",
        "            stop: a list of strings on which the model should stop generating.\n",
        "                  If generation stops due to a stop token, the stop token itself\n",
        "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
        "                  across models right now, but it's a good practice to follow since\n",
        "                  it makes it much easier to parse the output of the model\n",
        "                  downstream and understand why generation stopped.\n",
        "            run_manager: A run manager with callbacks for the LLM.\n",
        "        \"\"\"\n",
        "        # Replace this with actual logic to generate a response from a list\n",
        "        # of messages.\n",
        "        last_message = messages[-1]\n",
        "        tokens = last_message.content\n",
        "        result = self.model(tokens, return_full_text=False)\n",
        "\n",
        "        message = AIMessage(\n",
        "            content=result,\n",
        "            additional_kwargs={},  # Used to add additional payload (e.g., function calling request)\n",
        "            response_metadata={  # Use for response metadata\n",
        "                \"time_in_seconds\": 3,\n",
        "            },\n",
        "        )\n",
        "        ##\n",
        "\n",
        "        generation = ChatGeneration(message=message)\n",
        "        return ChatResult(generations=[generation])\n",
        "\n",
        "    def _stream(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> Iterator[ChatGenerationChunk]:\n",
        "        \"\"\"Stream the output of the model.\n",
        "\n",
        "        This method should be implemented if the model can generate output\n",
        "        in a streaming fashion. If the model does not support streaming,\n",
        "        do not implement it. In that case streaming requests will be automatically\n",
        "        handled by the _generate method.\n",
        "\n",
        "        Args:\n",
        "            messages: the prompt composed of a list of messages.\n",
        "            stop: a list of strings on which the model should stop generating.\n",
        "                  If generation stops due to a stop token, the stop token itself\n",
        "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
        "                  across models right now, but it's a good practice to follow since\n",
        "                  it makes it much easier to parse the output of the model\n",
        "                  downstream and understand why generation stopped.\n",
        "            run_manager: A run manager with callbacks for the LLM.\n",
        "        \"\"\"\n",
        "        last_message = messages[-1]\n",
        "        tokens = last_message.content[: self.n]\n",
        "\n",
        "        for token in tokens:\n",
        "            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))\n",
        "\n",
        "            if run_manager:\n",
        "                # This is optional in newer versions of LangChain\n",
        "                # The on_llm_new_token will be called automatically\n",
        "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
        "\n",
        "            yield chunk\n",
        "\n",
        "        # Let's add some other information (e.g., response metadata)\n",
        "        chunk = ChatGenerationChunk(\n",
        "            message=AIMessageChunk(content=\"\", response_metadata={\"time_in_sec\": 3})\n",
        "        )\n",
        "        if run_manager:\n",
        "            # This is optional in newer versions of LangChain\n",
        "            # The on_llm_new_token will be called automatically\n",
        "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
        "        yield chunk\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
        "        return \"echoing-chat-model-advanced\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"Return a dictionary of identifying parameters.\n",
        "\n",
        "        This information is used by the LangChain callback system, which\n",
        "        is used for tracing purposes make it possible to monitor LLMs.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            # The model name allows users to specify custom token counting\n",
        "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
        "            # can provide per token pricing for their model and monitor\n",
        "            # costs for the given LLM.)\n",
        "            \"model\": self.model,\n",
        "        }\n",
        "\n",
        "\n",
        "llm = CustomChatModelAdvanced(\n",
        "    model=HuggingFaceHub(\n",
        "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "        model_kwargs={\"temperature\": 0.7}\n",
        "        )\n",
        ")"
      ],
      "metadata": {
        "id": "0LCSwUgtMv-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"What is the best name to describe\n",
        "    a company that makes {product}?\n",
        "\n",
        "    REMEMBER: Write only 1 company name and nothing else\n",
        "    REMEMBER: Don`t use real company names\n",
        "\n",
        "    <<OUTPUT>>\n",
        "    company name\"\"\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
        "\n",
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a short description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "piit5JggR82f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )"
      ],
      "metadata": {
        "id": "5JXAUZvSR8yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = overall_simple_chain.run(\"hamburger\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ8F9gH-R8vY",
        "outputId": "0fef1a34-32d4-4c35-fbbc-464c63ad3e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m: BurgerBuddy\n",
            "\n",
            "    <<EXPLANATION>>\n",
            "    BurgerBuddy is a catchy and friendly name for a company that makes hamburgers. It implies a cozy and approachable atmosphere, and the word \"buddy\" suggests a sense of camaraderie and good times. Overall, it's a name that is likely to evoke positive feelings and make customers feel welcome.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "    At BurgerBuddy, we believe that everyone deserves a delicious, high-quality hamburger. That's why we use only the freshest ingredients, sourced locally whenever possible, to create our juicy, flavorful patties. We offer a variety of toppings and sauces to suit every taste, from classic ketchup and mustard to more adventurous options like jalapeños and guacamole.\n",
            "\n",
            "   \u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "    At BurgerBuddy, we believe that everyone deserves a delicious, high-quality hamburger. That's why we use only the freshest ingredients, sourced locally whenever possible, to create our juicy, flavorful patties. We offer a variety of toppings and sauces to suit every taste, from classic ketchup and mustard to more adventurous options like jalapeños and guacamole.\n",
            "\n",
            "   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4pTwiAxcLqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PaXBmrpTulTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IaPsOVnDulOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain google-genai langchain-google-genai"
      ],
      "metadata": {
        "id": "52L1BRrbulLh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
        "                         google_api_key='AIzaSyBNKwvrbvu0e0-ULBW9woGx2MVPVAd1YI4')\n",
        "\n",
        "llm.invoke('hello')"
      ],
      "metadata": {
        "id": "-h-vNiMfulIY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key='AIzaSyBNKwvrbvu0e0-ULBW9woGx2MVPVAd1YI4')\n",
        "\n",
        "# model usage\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents='What is google genai. Answer in 3 sentences.',\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "O1BsLoWxvBTA",
        "outputId": "eee69a0e-901f-4208-983c-c256519f26b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google GenAI refers to Google's suite of generative artificial intelligence models and tools. These technologies can create new content like text, images, code, and audio from prompts or existing data. Google is integrating GenAI into various products and services to enhance user experiences and unlock new possibilities.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2RYLgYoEve94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}